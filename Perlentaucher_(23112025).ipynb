{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPUUsbupKuAV3HKu7ZpGEs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnmarten/utils/blob/main/Perlentaucher_(23112025).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change book url in script before running\n",
        "#results will be downloaded as csv file (allow downloads)\n",
        "test_url = 'https://www.perlentaucher.de/buch/mareike-fallwickl/und-alle-so-still.html'"
      ],
      "metadata": {
        "id": "9aU5R-TfvFIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-Cmatri1Jpp"
      },
      "outputs": [],
      "source": [
        "#perlentaucher 2311\n",
        "\n",
        "# Update package list and install necessary dependencies\n",
        "!apt-get update\n",
        "!apt-get install -y wget unzip libvulkan1\n",
        "\n",
        "# Download and install Google Chrome\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -f -y\n",
        "\n",
        "# Install xvfb for virtual framebuffer support\n",
        "!apt-get install -y xvfb\n",
        "\n",
        "# Install required packages\n",
        "!pip install selenium chromedriver-autoinstaller beautifulsoup4 pandas\n",
        "\n",
        "# Automatically install the correct version of ChromeDriver\n",
        "import chromedriver_autoinstaller\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def create_driver():\n",
        "    \"\"\"Create Chrome driver configured for Colab headless environment\"\"\"\n",
        "    options = Options()\n",
        "\n",
        "    # Colab/headless specific options\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument('--window-size=1920,1080')\n",
        "    options.add_argument('--remote-debugging-port=9222')\n",
        "\n",
        "    # Anti-detection measures\n",
        "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "    # Add user agent to appear as a real browser\n",
        "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
        "\n",
        "    # Additional headers\n",
        "    options.add_argument('--accept-language=en-US,en;q=0.9')\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    # Execute CDP commands to hide automation\n",
        "    driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n",
        "        \"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
        "    })\n",
        "\n",
        "    # Remove webdriver property\n",
        "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "\n",
        "    return driver\n",
        "\n",
        "def scrape_book_urls(max_pages=425):\n",
        "    \"\"\"Scrape all book URLs from the listing pages\"\"\"\n",
        "    driver = create_driver()\n",
        "    book_urls = []\n",
        "\n",
        "    try:\n",
        "        driver.get('https://www.perlentaucher.de/buchKSL/deutsche-romane.html')\n",
        "        print(\"Starting to scrape book URLs...\")\n",
        "\n",
        "        for i in range(max_pages):\n",
        "            try:\n",
        "                # Wait for the book links to load\n",
        "                WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, '.book.teaser-block h3 a'))\n",
        "                )\n",
        "\n",
        "                # Get all book URLs on current page\n",
        "                links = driver.find_elements(By.CSS_SELECTOR, '.book.teaser-block h3 a')\n",
        "                current_urls = [link.get_attribute('href') for link in links]\n",
        "                book_urls.extend(current_urls)\n",
        "\n",
        "                print(f\"Page {i+1}: Found {len(current_urls)} books. Total: {len(book_urls)}\")\n",
        "\n",
        "                # Try to click next page button\n",
        "                try:\n",
        "                    next_button = WebDriverWait(driver, 5).until(\n",
        "                        EC.element_to_be_clickable((By.CSS_SELECTOR, '.related-next'))\n",
        "                    )\n",
        "                    next_button.click()\n",
        "                    time.sleep(2)  # Wait for page to load\n",
        "                except:\n",
        "                    print(f\"No more pages found after page {i+1}\")\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error on page {i+1}: {e}\")\n",
        "                break\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    # Save URLs to file\n",
        "    with open('BooksUrls.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(book_urls))\n",
        "\n",
        "    print(f\"\\nTotal books scraped: {len(book_urls)}\")\n",
        "    return book_urls\n",
        "\n",
        "def scrape_review_content(review_link):\n",
        "    \"\"\"Scrape the actual review text from a review page\"\"\"\n",
        "    driver = create_driver()\n",
        "    review_content = \"\"\n",
        "\n",
        "    try:\n",
        "        driver.get(review_link)\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "        # Look for review content in span elements within the review section\n",
        "        # Try multiple possible selectors\n",
        "        review_spans = soup.select('span.review-text, .review-content span, .text span, span')\n",
        "\n",
        "        # If that doesn't work, try getting all text from common review containers\n",
        "        if not review_spans:\n",
        "            review_container = soup.select_one('.review-text, .review-content, .text, .content')\n",
        "            if review_container:\n",
        "                review_content = review_container.get_text(strip=True)\n",
        "        else:\n",
        "            # Join all span texts\n",
        "            review_content = ' '.join([span.get_text(strip=True) for span in review_spans if span.get_text(strip=True)])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ⚠ Could not fetch review content: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    return review_content\n",
        "\n",
        "def scrape_book_details(url):\n",
        "    \"\"\"Scrape details and reviews from a single book page\"\"\"\n",
        "    driver = create_driver()\n",
        "    book_data = {\n",
        "        'url': url,\n",
        "        'title': None,\n",
        "        'author': None,\n",
        "        'description': None,\n",
        "        'publisher': None,\n",
        "        'year': None,\n",
        "        'reviews': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Get page source and parse with BeautifulSoup\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "        # Extract title\n",
        "        title_elem = soup.select_one('h1')\n",
        "        if title_elem:\n",
        "            book_data['title'] = title_elem.get_text(strip=True)\n",
        "\n",
        "        # Extract author\n",
        "        author_elem = soup.select_one('.author, .book-author')\n",
        "        if author_elem:\n",
        "            book_data['author'] = author_elem.get_text(strip=True)\n",
        "\n",
        "        # Extract description\n",
        "        desc_elem = soup.select_one('.book-description, .description, .text')\n",
        "        if desc_elem:\n",
        "            book_data['description'] = desc_elem.get_text(strip=True)\n",
        "\n",
        "        # Extract publisher and year\n",
        "        meta_elem = soup.select_one('.book-meta, .meta')\n",
        "        if meta_elem:\n",
        "            meta_text = meta_elem.get_text(strip=True)\n",
        "            book_data['publisher'] = meta_text\n",
        "\n",
        "        # Extract reviews - look for h3.newspaper elements\n",
        "        review_headers = soup.select('h3.newspaper')\n",
        "        print(f\"  Found {len(review_headers)} review headers\")\n",
        "\n",
        "        for idx, header in enumerate(review_headers, 1):\n",
        "            review_header_text = header.get_text(strip=True)\n",
        "\n",
        "            # Get the review link\n",
        "            link = header.select_one('a.newspaper')\n",
        "            if link:\n",
        "                review_link = link.get('href', '')\n",
        "                if review_link.startswith('/'):\n",
        "                    review_link = f\"https://www.perlentaucher.de{review_link}\"\n",
        "\n",
        "                print(f\"  Fetching review {idx}/{len(review_headers)} content from: {review_link}\")\n",
        "                # Scrape the actual review content\n",
        "                review_content = scrape_review_content(review_link)\n",
        "\n",
        "                book_data['reviews'].append({\n",
        "                    'source': review_header_text,\n",
        "                    'content': review_content\n",
        "                })\n",
        "            else:\n",
        "                # No link, just save the header\n",
        "                book_data['reviews'].append({\n",
        "                    'source': review_header_text,\n",
        "                    'content': ''\n",
        "                })\n",
        "\n",
        "        print(f\"✓ Scraped: {book_data['title']} - {len(book_data['reviews'])} reviews with content\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error scraping {url}: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    return book_data\n",
        "\n",
        "def scrape_multiple_books(urls, delay=2):\n",
        "    \"\"\"Scrape details for multiple books\"\"\"\n",
        "    all_books_data = []\n",
        "\n",
        "    for idx, url in enumerate(urls, 1):\n",
        "        print(f\"\\nScraping book {idx}/{len(urls)}: {url}\")\n",
        "        try:\n",
        "            book_data = scrape_book_details(url)\n",
        "            all_books_data.append(book_data)\n",
        "            time.sleep(delay)  # Be respectful with requests\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape book {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_books_data\n",
        "\n",
        "def save_to_csv(books_data, filename='books_with_reviews.csv'):\n",
        "    \"\"\"Save scraped data to CSV and auto-download in Colab\"\"\"\n",
        "    rows = []\n",
        "    for book in books_data:\n",
        "        # Create separate rows for each review\n",
        "        if book['reviews']:\n",
        "            for review in book['reviews']:\n",
        "                rows.append({\n",
        "                    'URL': book['url'],\n",
        "                    'Title': book['title'],\n",
        "                    'Author': book['author'],\n",
        "                    'Description': book['description'],\n",
        "                    'Publisher': book['publisher'],\n",
        "                    'Year': book['year'],\n",
        "                    'Review Source': review['source'],\n",
        "                    'Review Content': review['content']\n",
        "                })\n",
        "        else:\n",
        "            # Add row even if no reviews\n",
        "            rows.append({\n",
        "                'URL': book['url'],\n",
        "                'Title': book['title'],\n",
        "                'Author': book['author'],\n",
        "                'Description': book['description'],\n",
        "                'Publisher': book['publisher'],\n",
        "                'Year': book['year'],\n",
        "                'Review Source': '',\n",
        "                'Review Content': ''\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(filename, index=False, encoding='utf-8')\n",
        "    print(f\"\\n✓ Data saved to {filename}\")\n",
        "    print(f\"  Total rows: {len(df)}\")\n",
        "\n",
        "    # Auto-download in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(filename)\n",
        "        print(f\"  ⬇ Downloaded {filename}\")\n",
        "    except:\n",
        "        print(f\"  (Not in Colab environment - file saved locally)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PERLENTAUCHER BOOK SCRAPER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Test with a single book\n",
        "print(\"\\n[TEST] Scraping single book...\")\n",
        "#test_url = 'https://www.perlentaucher.de/buch/mareike-fallwickl/und-alle-so-still.html'\n",
        "book_data = scrape_book_details(test_url)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESULTS:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Title: {book_data['title']}\")\n",
        "print(f\"Author: {book_data['author']}\")\n",
        "print(f\"Reviews found: {len(book_data['reviews'])}\")\n",
        "if book_data['reviews']:\n",
        "    print(\"\\nReviews:\")\n",
        "    for i, review in enumerate(book_data['reviews'], 1):\n",
        "        print(f\"\\n  {i}. Source: {review['source']}\")\n",
        "        print(f\"     Content preview: {review['content'][:200]}...\" if len(review['content']) > 200 else f\"     Content: {review['content']}\")\n",
        "else:\n",
        "    print(\"  No reviews found\")\n",
        "\n",
        "# Auto-save single test result to CSV\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Saving test result to CSV...\")\n",
        "df = save_to_csv([book_data], filename='single_book_test.csv')\n",
        "print(df)\n",
        "\n",
        "# Step 2: Uncomment to scrape all book URLs (may take a long time!)\n",
        "# print(\"\\n[STEP 2] Scraping all book URLs...\")\n",
        "# book_urls = scrape_book_urls(max_pages=5)  # Adjust max_pages as needed\n",
        "\n",
        "# Step 3: Uncomment to load URLs from file\n",
        "# print(\"\\n[STEP 3] Loading URLs from file...\")\n",
        "# with open('BooksUrls.txt', 'r', encoding='utf-8') as f:\n",
        "#     book_urls = [line.strip() for line in f.readlines() if line.strip()]\n",
        "# print(f\"Loaded {len(book_urls)} URLs\")\n",
        "\n",
        "# Step 4: Uncomment to scrape multiple books\n",
        "# print(\"\\n[STEP 4] Scraping multiple books...\")\n",
        "# books_to_scrape = book_urls[:10]  # Test with first 10 books\n",
        "# all_data = scrape_multiple_books(books_to_scrape, delay=2)\n",
        "# df = save_to_csv(all_data)\n",
        "# print(\"\\nFirst few rows:\")\n",
        "# print(df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DONE! Uncomment sections above to scrape more books.\")\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}