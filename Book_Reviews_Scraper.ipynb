{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "Requirement already satisfied: six in /home/gnmarten/anaconda3/lib/python3.8/site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=ed3d7cd29e2da3a7882b8a117c4ef4f6e2da550b8e91d6fa2ea197c7d5c2105c\n",
      "  Stored in directory: /home/gnmarten/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langdetect import detect\n",
    "import re\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports here\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 111.0.5563\n",
      "[WDM] - Get LATEST chromedriver version for 111.0.5563 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\gnmarten\\.wdm\\drivers\\chromedriver\\win32\\111.0.5563.64\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "#automatically install web driver using the webdriver_manager library\n",
    "#presumes Chrome is on your system\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Top 100 Book Titles and Links to Webpage for Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating array from Times Top 100 Books of All Time webpage on Goodreads website\n",
    "\n",
    "TimesTop100  = requests.get(\"https://www.goodreads.com/list/show/187015.Neurodiversity_related\").content\n",
    "\n",
    "## Grabbing all tags in webpage of 'a' type and class 'bookTitle'\n",
    "soup = BeautifulSoup(TimesTop100,\"lxml\")\n",
    "cont = soup.select(\"a.bookTitle\")\n",
    "\n",
    "## Iterating through and creating list for all titles (bookT) and links (bookLink)\n",
    "bookT = [x.text.strip() for x in cont]\n",
    "bookLink = ['https://www.goodreads.com'+ x.get('href') for x in cont]\n",
    "\n",
    "## Combining list\n",
    "con = np.column_stack((bookT, bookLink))\n",
    "Top100Books = pd.DataFrame(con, columns = ['Book Title', 'Book Link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Book Review Page Links and Unqiue ID for Book in Goodreads Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieving Page Link for the reviews of a certain book\n",
    "# requests.get has tendency to crash so must be run with a while loop until requests successfully works - load time (~11.5 min)\n",
    "\n",
    "ReviewPageLink = []\n",
    "for i in bookLink:\n",
    "    cont = None\n",
    "    while cont == None:\n",
    "            r = requests.get(i).content\n",
    "            soup = BeautifulSoup(r,\"lxml\")\n",
    "            cont = soup.find(\"a\", attrs = {'class' : 'Button Button--transparent Button--small'})\n",
    "\n",
    "    ReviewPageLink += ['https://www.goodreads.com' + cont['href']]\n",
    "\n",
    "# Finding the Unique ID set by Goodreads in each books main page href\n",
    "BookID = [re.search('\\d+', i)[0] for i in Top100Books['Book Link']]\n",
    "\n",
    "# adding information to dataframe\n",
    "Top100Books['Review Page Link'] = ReviewPageLink\n",
    "Top100Books['Unique ID'] = BookID\n",
    "\n",
    "# saving dataframe to csv file\n",
    "Top100Books.to_csv('Top100BooksData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining function to continuing spamming requests until html can be processed\n",
    "## when looking to iterate through multiple links while scraping\n",
    "\n",
    "# Code currently returns a search - however - might be more important to return just the soup \n",
    "# Therefore, it can be used for mulitple searches if required\n",
    "# The caveat is that by the definition of the function it must find some object that does exist on the page regardless\n",
    "# in order to work \n",
    "\n",
    "def spamRequestsFind(link, tag, attr, attr_id ):\n",
    "    status = None\n",
    "    while status == None:\n",
    "        r = requests.get(link).content\n",
    "        soup = BeautifulSoup(r, \"lxml\")\n",
    "        status = type(soup.find(tag, attrs = {attr : attr_id}))\n",
    "\n",
    "    return(soup.find(tag, attrs = {attr : attr_id}))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Book Link</th>\n",
       "      <th>Review Page Link</th>\n",
       "      <th>Unique ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bewilderment</td>\n",
       "      <td>https://www.goodreads.com/book/show/56404444-b...</td>\n",
       "      <td>https://www.goodreads.com/book/show/56404444/r...</td>\n",
       "      <td>56404444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Book Title                                          Book Link  \\\n",
       "0  Bewilderment  https://www.goodreads.com/book/show/56404444-b...   \n",
       "\n",
       "                                    Review Page Link  Unique ID  \n",
       "0  https://www.goodreads.com/book/show/56404444/r...   56404444  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Top100Books = pd.read_csv('Top100BooksData.csv')\n",
    "Top100Books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MultBookReviews grabs all defined information for all book reviews loaded on the page in browser\n",
    "\n",
    "def MultBookReviews(page_source):\n",
    "\n",
    "    ## starting by grabbing one persons review and information\n",
    "    \n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    ReviewCards = soup.find_all('article', attrs = {'class' : 'ReviewCard'})\n",
    "\n",
    "    ## Book Title and Author\n",
    "    title = soup.find('h1', attrs = {'class' : 'Text H1Title'}).text\n",
    "    author = soup.find('h3', attrs = {'class' : 'Text Text__title3 Text__regular'}).text\n",
    "\n",
    "    ## List of all user account hrefs for account page\n",
    "    cont = soup.select('div.ReviewerProfile__name')\n",
    "    hrefsUsers = [x.find('a')['href'] for x in cont]\n",
    "\n",
    "    ## Text data of user review\n",
    "    contReview = soup.select(\"section.ReviewText\")\n",
    "    Reviews = [x.text.strip() for x in contReview]\n",
    "\n",
    "    ## grabbing individual user rating for review \n",
    "    contRatingCont = soup.select(\"div.ShelfStatus\")\n",
    "    userRatings = [x.find('span')['aria-label'] if (x.findChildren('span', recursive=False) == []) == False else 'No Rating' for x in contRatingCont]\n",
    "\n",
    "    ## Date the review was written by user\n",
    "    dateCont = soup.select('section.ReviewCard__row')\n",
    "    datesOfReviews = [x.find('span', attrs = {'class': 'Text Text__body3'}).text for x in dateCont]\n",
    "\n",
    "    ## Amount of likes and comments for review\n",
    "    commentLikeCont = soup.select('footer.SocialFooter')\n",
    "    likes = ['0' if x.find('div', attrs={'class': 'SocialFooter__statsContainer'}) == None else x.find('span', attrs={'class': 'Button__labelItem'}).text  for x in commentLikeCont]\n",
    "    comments = ['0' if x.find('div', attrs={'class': 'Button__container'}).next_sibling == None else x.find('div', attrs={'class': 'Button__container'}).next_sibling.text for x in commentLikeCont]\n",
    "\n",
    "    ### Creating DataFrame of all the review data\n",
    "\n",
    "    reviewData = pd.DataFrame({ 'User Href' : hrefsUsers,\n",
    "                                'Title' : title,\n",
    "                                'Rating' : userRatings,\n",
    "                                'Date' : datesOfReviews,\n",
    "                                'Likes' : likes,\n",
    "                                'Comments' : comments,\n",
    "                                'Review' : Reviews})\n",
    "    \n",
    "    return(reviewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Function to Automate clicking \"show more reviews\" on each book webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## getManyReviews uses selenium to load more reviews on a book review webpage\n",
    "## Each iteration clicks the \"show more reviews\" button (loads 30 more reviews per click) up to a cap of 36 clicks\n",
    "## 36 was determined as the cap before the memory of a browser page would be expended resulting in a crash\n",
    "\n",
    "def getManyReviews(url):\n",
    "    clicks = 0\n",
    "\n",
    "    # Initilaizing driver and webpage and allowing time for reviews to load\n",
    "    #driver = webdriver.Edge()\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    nreviews = int(re.sub('\\D', '', soup.find('span', attrs = {'class' : 'Text Text__body3 Text__subdued'}).text))\n",
    "    cap = 36\n",
    "    iters = np.round(nreviews/30)-1\n",
    "\n",
    "    if iters < cap:\n",
    "        while clicks < iters:\n",
    "\n",
    "            # scrolling down page to ensure click will work on \"show more results\" button\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Clicking \"show more results\" button\n",
    "            SMResults = driver.find_element(By.XPATH, \"//div[@class = 'Divider Divider--contents Divider--largeMargin']/div[@class = 'Button__container']/button\")\n",
    "            driver.execute_script(\"arguments[0].click();\", SMResults)\n",
    "            time.sleep(1)\n",
    "\n",
    "            clicks += 1\n",
    "    else:\n",
    "        while clicks < cap:\n",
    "\n",
    "            # scrolling down page to ensure click will work on \"show more results\" button\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Clicking \"show more results\" button\n",
    "            SMResults = driver.find_element(By.XPATH, \"//div[@class = 'Divider Divider--contents Divider--largeMargin']/div[@class = 'Button__container']/button\")\n",
    "            driver.execute_script(\"arguments[0].click();\", SMResults)\n",
    "            time.sleep(1)\n",
    "\n",
    "            clicks += 1\n",
    "\n",
    "    # grabbing reference for final state of page after n number of \"show more results\" button clicks\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    reviews = MultBookReviews(page_source)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating Through Batches of Books to Grab Reviews (5 books, ~1000 reviews per book, ~15 min to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\python36\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 1 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-bf7550d5010e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Processing in smaller batches to double check loading into csv was successful\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mbookdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetManyReviews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTop100Books\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Review Page Link'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mbookdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Book{num}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\python36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\python36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\python36\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    355\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Processing in smaller batches to double check loading into csv was successful\n",
    "for i in range(0, 5):\n",
    "    bookdata = getManyReviews(Top100Books['Review Page Link'][i])\n",
    "    bookdata.to_csv('Book{num}.csv'.format(num=i+1), index=False)\n",
    "#will exit on valuerror 1 is not in range because there is only 1 book, but the CSV will be saved a Book1.csv regardless"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
